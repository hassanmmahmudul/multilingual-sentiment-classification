{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/TurkuNLP/Deep_Learning_in_LangTech_course/blob/master/course_project_template.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CD3_cBZ4SFaz"
   },
   "source": [
    "# Multilingual Sentiment Classification using BERT & LLMs\n",
    "\n",
    "- Chosen Corpus: [Multilingual Amazon Reviews Corpus (MARC)](https://registry.opendata.aws/amazon-reviews-ml/)\n",
    "\n",
    "### Corpus information\n",
    "\n",
    "<!-- - Description of the chosen corpus:\n",
    "- Paper(s) and other published materials related to the corpus:\n",
    "- Random baseline performance and expected performance for recent machine learned models:\n",
    "   -->\n",
    "#### Description of the Chosen Corpus\n",
    "\n",
    "The [Multilingual Amazon Reviews Corpus (MARC)](https://registry.opendata.aws/amazon-reviews-ml/) is a large-scale dataset tailored for multilingual text classification. It comprises Amazon reviews in six languages: English, Japanese, German, French, Spanish, and Chinese, collected between 2015 and 2019. The dataset includes *review text*, *review titles*, *star ratings*, *anonymized reviewer* and *product IDs*, and *coarse-grained product categories*. Each language contains balanced subsets of 200,000 training samples, 5,000 development samples, and 5,000 test samples, ensuring equal representation of the five star ratings. The reviews are filtered for quality through language detection algorithms and additional criteria such as verified purchases and token frequency thresholds.\n",
    "\n",
    "The dataset was made publicly available in Amazon’s [Open Data Program](https://registry.opendata.aws/amazon-reviews-ml/), specifically stored in Amazon Simple Storage Service (S3). Also it is available on [Hugging Face](https://huggingface.co/datasets/mteb/amazon_reviews_multi/) with some extension. On Hugging Face, the dataset is reorganized to include translations of the reviews into multiple languages, allowing multilingual evaluations. This version follows a standardized format, where each row contains an `id`, a unique identifier for the review (e.g `de_0203609`). It also includes `text`, which is the text of the review, either in its original language or translated. The `label` ranging from 0 to 4, derived from the star ratings, with 0 indicating the lowest star rating (1 star), and 4 indicating highest sentiment (5 star). Additionally, the dataset includes the `label_text`, which is the textual representation of the sentiment class. The Hugging Face implementation of the dataset integrates it with the [Massive Text Embedding Benchmark (MTEB)]() framework, facilitating benchmarking and model evaluations across various languages and tasks such as zero-shot transfer or multilingual evaluations.\n",
    "\n",
    "---\n",
    "\n",
    "#### Paper(s) and Other Published Materials Related to the Corpus\n",
    "\n",
    "The primary reference for the dataset is the [paper](https://aclanthology.org/2020.emnlp-main.369/) titled The Multilingual Amazon Reviews Corpus, presented at the [2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)](https://2020.emnlp.org/). The authors (Phillip Keung et al.) highlight the dataset's construction process, its utility for supervised and zero-shot cross-lingual tasks, and baseline results using multilingual BERT (mBERT). Related research includes prior multilingual datasets such as Reuters RCV1/RCV2, XNLI, and earlier Amazon review datasets (e.g., Ni et al., 2019), which were limited by smaller sizes, lack of multilingual focus, or absence of well-defined splits. The MARC addresses these gaps by offering a balanced, comprehensive and publicly dataset for reproducible multilingual NLP research. Furthermore, the paper emphasizes that the dataset allows researchers to evaluate models in *zero-shot transfer learning scenarios*, where a model trained on one language is tested on another.\n",
    "\n",
    "---\n",
    "\n",
    "#### Random Baseline Performance and Expected Performance for Recent Machine Learned Models\n",
    "\n",
    "While favorable for the metric *accuracy* widely used for classification task, the original paper recommended the metric *mean absolute error (MAE)*. Given the five-star rating system, random baseline performance for MARC would yield:\n",
    "- **Accuracy**: $ \\frac{1}{5} = 20\\% $ since there are five equally likely star ratings.\n",
    "- **Mean Absolute Error (MAE)**: Approximately 2.0, assuming predictions are uniformly distributed across the star ratings.\n",
    "\n",
    "Recent machine-learned models like fine-tuned mBERT demonstrate significant improvements:\n",
    "1. **Fully Supervised Fine-Grained Classification**:\n",
    "   - **Accuracy**: On average, mBERT achieves an accuracy of **59.2%** when using the review body combined with title and product category as inputs.\n",
    "   - **MAE**: An average of **0.482** for the same setup.\n",
    "\n",
    "2. **Zero-Shot Cross-Lingual Transfer (Fine-Grained Classification)**:\n",
    "   - **Accuracy**: An average of **44.0%** when trained on one source language and tested on others.\n",
    "   - **MAE**: An average of **0.769**, demonstrating its effectiveness across languages even without direct training data.\n",
    "\n",
    "\n",
    "\n",
    "<!--\n",
    "##### Random Baseline Performance:\n",
    "From the paper, the **random baseline** performance for this dataset aligns with the number of classes (five sentiment labels). In a balanced dataset, a random classifier would have an accuracy of **20%**. However, due to class imbalance (e.g., positive reviews being more frequent), the actual random baseline might slightly deviate from this.\n",
    "\n",
    "##### Expected Performance for Machine-Learned Models:\n",
    "Highlighted sections in the paper describe the performance of modern multilingual models on this dataset:\n",
    "1. **mBERT and XLM-R**: Models like **mBERT** and **XLM-R (XLM-RoBERTa)** demonstrate state-of-the-art performance, achieving classification accuracies above **85%** for single-language tasks. In **cross-lingual evaluations**, where models are trained on one language and tested on another, performance slightly decreases but remains significantly higher than random baselines.\n",
    "2. **Scaling with Dataset Size**: The paper emphasizes that larger datasets and pre-trained language models significantly enhance performance, making this dataset ideal for fine-tuning tasks. -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D5d-9uxrcDY-"
   },
   "source": [
    "---\n",
    "\n",
    "## 1. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-15T05:18:55.995849Z",
     "iopub.status.busy": "2024-12-15T05:18:55.995527Z",
     "iopub.status.idle": "2024-12-15T05:19:21.207671Z",
     "shell.execute_reply": "2024-12-15T05:19:21.206577Z",
     "shell.execute_reply.started": "2024-12-15T05:18:55.995821Z"
    },
    "id": "rKol6wOaKT2r",
    "outputId": "bf871ddf-0809-4917-cc6d-1ea46ee940c4"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/pty.py:89: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
      "  pid, fd = os.forkpty()\n"
     ]
    }
   ],
   "source": [
    "#Install libraries\n",
    "!pip install --quiet datasets evaluate\n",
    "!pip install -U bitsandbytes --quiet\n",
    "!pip install --quiet transformers accelerate #bitsandbytes>0.37.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-15T05:20:36.558643Z",
     "iopub.status.busy": "2024-12-15T05:20:36.558257Z",
     "iopub.status.idle": "2024-12-15T05:20:36.563967Z",
     "shell.execute_reply": "2024-12-15T05:20:36.563043Z",
     "shell.execute_reply.started": "2024-12-15T05:20:36.558603Z"
    },
    "id": "WkeOgl1dVorE"
   },
   "outputs": [],
   "source": [
    "#Import librairies\n",
    "from datasets import load_dataset, Dataset, concatenate_datasets\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer, EarlyStoppingCallback, AutoModelForCausalLM, pipeline, BitsAndBytesConfig\n",
    "import evaluate\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "import numpy as np\n",
    "import re\n",
    "import torch\n",
    "import os\n",
    "\n",
    "# To ignore warinings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ovUapilSb8iT"
   },
   "source": [
    "---\n",
    "\n",
    "## 2. Data download, sampling and preprocessing\n",
    "\n",
    "### 2.1. Download the corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tTrujcMS81wX"
   },
   "source": [
    "In this project, we consider the English, French, German and Spanish corpora of the MARC dataset loaded from Hugging Face. The remaining of this project uses MARC loaded from Hugging Face."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "referenced_widgets": [
      "b57a13bc9aed4a11a36e28517427138f",
      "6a59366dbbe84720ab2eb1b3a03293ea",
      "7bf9491817834d5da2cdfc23b143dfb0",
      "e5712b7f483f48f0b2c0245f29fcf0b1",
      "d91b3403ade747b792b080a7777633e9",
      "30d1502a6f724386986a5a55bf493c7e",
      "ba220541f7d74a62995b23029a9a3f80",
      "88ea2f560a0b4e2b9a9a94127c8a4421",
      "340a90f23b31460592aa95769fe8c801",
      "8b1d4ae568764f4ab22bbe8c5331c2f7",
      "1723fce27a134451a34d06c74dc01b93",
      "bf3960fae6894380855b54a0ee9db81e",
      "d399e2bac1ff489aa49217025ee8ed4a",
      "5512c9061c4f476687c69201e1f17bcd",
      "f2f7ab5a23734db384b754b9ee967fd9",
      "d5918fbd12754f2dbac4d0453b8eaed8",
      "8fc6f0458e0f46179fa4ffaca0fb83f1",
      "902e8b8091324f328cbd35d339d6e595",
      "efc1f946cbbb4e5fbd312aff5172a1a2",
      "cec0af43b59d4b348176d5086c886410",
      "39b9b44d7d0d4a07a3ce7730c89e64f0",
      "79c097c7a759483c8621f6ecb6f0294d",
      "85eea506cf89433e9bc9fbb80410b282",
      "e04025809fc74208862a1ee290d5066c",
      "68ff575409364c0195bd606af54ef1ac",
      "bfb4aca78cc04052a82b3140400f2b0f"
     ]
    },
    "execution": {
     "iopub.execute_input": "2024-12-14T23:36:05.347685Z",
     "iopub.status.busy": "2024-12-14T23:36:05.347372Z",
     "iopub.status.idle": "2024-12-14T23:36:16.398587Z",
     "shell.execute_reply": "2024-12-14T23:36:16.397702Z",
     "shell.execute_reply.started": "2024-12-14T23:36:05.347623Z"
    },
    "id": "uFGVqcoD81wX",
    "outputId": "324e1e13-e56c-4f50-87c9-f740ff8d605f"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b57a13bc9aed4a11a36e28517427138f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/47.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6a59366dbbe84720ab2eb1b3a03293ea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "amazon_reviews_multi.py:   0%|          | 0.00/6.17k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7bf9491817834d5da2cdfc23b143dfb0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0000.parquet:   0%|          | 0.00/28.3M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e5712b7f483f48f0b2c0245f29fcf0b1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "en/validation/0000.parquet:   0%|          | 0.00/713k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d91b3403ade747b792b080a7777633e9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "en/test/0000.parquet:   0%|          | 0.00/711k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "30d1502a6f724386986a5a55bf493c7e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/200000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ba220541f7d74a62995b23029a9a3f80",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split:   0%|          | 0/5000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "88ea2f560a0b4e2b9a9a94127c8a4421",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/5000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "340a90f23b31460592aa95769fe8c801",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0000.parquet:   0%|          | 0.00/25.7M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8b1d4ae568764f4ab22bbe8c5331c2f7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "fr/validation/0000.parquet:   0%|          | 0.00/627k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1723fce27a134451a34d06c74dc01b93",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "fr/test/0000.parquet:   0%|          | 0.00/642k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bf3960fae6894380855b54a0ee9db81e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/200000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d399e2bac1ff489aa49217025ee8ed4a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split:   0%|          | 0/5000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5512c9061c4f476687c69201e1f17bcd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/5000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f2f7ab5a23734db384b754b9ee967fd9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0000.parquet:   0%|          | 0.00/31.9M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d5918fbd12754f2dbac4d0453b8eaed8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "de/validation/0000.parquet:   0%|          | 0.00/800k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8fc6f0458e0f46179fa4ffaca0fb83f1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "de/test/0000.parquet:   0%|          | 0.00/797k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "902e8b8091324f328cbd35d339d6e595",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/200000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "efc1f946cbbb4e5fbd312aff5172a1a2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split:   0%|          | 0/5000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cec0af43b59d4b348176d5086c886410",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/5000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "39b9b44d7d0d4a07a3ce7730c89e64f0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0000.parquet:   0%|          | 0.00/24.6M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "79c097c7a759483c8621f6ecb6f0294d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "es/validation/0000.parquet:   0%|          | 0.00/613k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "85eea506cf89433e9bc9fbb80410b282",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "es/test/0000.parquet:   0%|          | 0.00/619k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e04025809fc74208862a1ee290d5066c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/200000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "68ff575409364c0195bd606af54ef1ac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split:   0%|          | 0/5000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bfb4aca78cc04052a82b3140400f2b0f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/5000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# code to download the corpus here\n",
    "\n",
    "#Load English corpus\n",
    "ds_en = load_dataset(\"mteb/amazon_reviews_multi\", \"en\")\n",
    "\n",
    "#Load French corpus\n",
    "ds_fr = load_dataset(\"mteb/amazon_reviews_multi\", \"fr\")\n",
    "\n",
    "#Load Deutch corpus\n",
    "ds_de = load_dataset(\"mteb/amazon_reviews_multi\", \"de\")\n",
    "\n",
    "# #Load Spanish corpus\n",
    "ds_es = load_dataset(\"mteb/amazon_reviews_multi\", \"es\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cXb7CQNCbZOI"
   },
   "source": [
    "### 2.2. Sampling and preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "q2Dhy5h581wY"
   },
   "source": [
    "As initially the dataset is balanced accros label and is already processed, we just consider the limited resource constraints, proceed to a downsampling and tokenization. We splitted the dataset into two parts: one consisting of English corpus and another consisting of non-English corpora.\n",
    "\n",
    "#### 2.2.1. Downsampling\n",
    "##### English corpus\n",
    "\n",
    "The English corpus is downsampled while ensuring balanced representation across labels. For the training set, 20,000 examples are sampled per label without replacement. For the test and validation sets, 100 examples per label are sampled. The English dataset has respectively 100000, 500 and 500 examples for training, validation and test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-15T05:29:33.624369Z",
     "iopub.status.busy": "2024-12-15T05:29:33.623658Z",
     "iopub.status.idle": "2024-12-15T05:29:40.918176Z",
     "shell.execute_reply": "2024-12-15T05:29:40.917510Z",
     "shell.execute_reply.started": "2024-12-15T05:29:33.624318Z"
    },
    "id": "QVfjJh7L81wY"
   },
   "outputs": [],
   "source": [
    "#Loading the English corpus as pandas dataframe\n",
    "en_train = pd.DataFrame(ds_en[\"train\"])\n",
    "en_test = pd.DataFrame(ds_en[\"test\"])\n",
    "en_val = pd.DataFrame(ds_en[\"validation\"])\n",
    "\n",
    "# We downsampled the dataset to 100000: we sample 20000 examples per label\n",
    "en_train = en_train.groupby('label').apply(lambda x: x.sample(n=20000, replace=False) if len(x) >= 20000 else x).reset_index(drop=True)\n",
    "# We downsampled the val set to 500: we sample 100 examples per label\n",
    "en_test = en_test.groupby('label').apply(lambda x: x.sample(n=100, replace=False) if len(x) >= 100 else x).reset_index(drop=True)\n",
    "en_test_copy = en_test#we keep this copy for testing the generative model after\n",
    "# We downsampled the test set to 500: we sample 100 examples per label\n",
    "en_val = en_val.groupby('label').apply(lambda x: x.sample(n=100, replace=False) if len(x) >= 100 else x).reset_index(drop=True)\n",
    "\n",
    "# Convert in Dataset object\n",
    "en_train = Dataset.from_pandas(en_train)\n",
    "en_test = Dataset.from_pandas(en_test)\n",
    "en_val = Dataset.from_pandas(en_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ADYVrX8f81wY"
   },
   "source": [
    "##### Non-English Data\n",
    "\n",
    "The non-English dataset is also downsampled while ensuring balanced representation across labels. For each of French, German and Spanish language, for the training set, 10,000 examples are sampled per label without replacement, and for the test and validation sets, 100 examples per label are sampled. The English dataset has respectively 150000, 1500 and 1500 examples for training, validation and test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-14T23:37:06.499420Z",
     "iopub.status.busy": "2024-12-14T23:37:06.498581Z",
     "iopub.status.idle": "2024-12-14T23:37:13.425971Z",
     "shell.execute_reply": "2024-12-14T23:37:13.425304Z",
     "shell.execute_reply.started": "2024-12-14T23:37:06.499370Z"
    },
    "id": "7wchQ5ju81wY"
   },
   "outputs": [],
   "source": [
    "#Loading the French corpus as pandas dataframe\n",
    "fr_train = pd.DataFrame(ds_fr[\"train\"])\n",
    "fr_test = pd.DataFrame(ds_fr[\"test\"])\n",
    "fr_val = pd.DataFrame(ds_fr[\"validation\"])\n",
    "\n",
    "\n",
    "# We downsampled the training set to 50000: we sample 10000 examples per label\n",
    "fr_train = fr_train.groupby('label').apply(lambda x: x.sample(n=10000, replace=False) if len(x) >= 10000 else x).reset_index(drop=True)\n",
    "# We downsampled the val set to 500: we sample 100 examples per label\n",
    "fr_test = fr_test.groupby('label').apply(lambda x: x.sample(n=100, replace=False) if len(x) >= 100 else x).reset_index(drop=True)\n",
    "# We downsampled the test set to 500: we sample 100 examples per label\n",
    "fr_val = fr_val.groupby('label').apply(lambda x: x.sample(n=100, replace=False) if len(x) >= 100 else x).reset_index(drop=True)\n",
    "\n",
    "#convert in dataset object\n",
    "# fr_train = Dataset.from_pandas(fr_train)\n",
    "fr_test = Dataset.from_pandas(fr_test)\n",
    "# fr_val = Dataset.from_pandas(fr_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-14T23:37:13.428394Z",
     "iopub.status.busy": "2024-12-14T23:37:13.427773Z",
     "iopub.status.idle": "2024-12-14T23:37:20.294617Z",
     "shell.execute_reply": "2024-12-14T23:37:20.293933Z",
     "shell.execute_reply.started": "2024-12-14T23:37:13.428354Z"
    },
    "id": "d5SKQRF_81wY"
   },
   "outputs": [],
   "source": [
    "#Loading the German corpus as pandas dataframe\n",
    "de_train = pd.DataFrame(ds_de[\"train\"])\n",
    "de_test = pd.DataFrame(ds_de[\"test\"])\n",
    "de_val = pd.DataFrame(ds_de[\"validation\"])\n",
    "\n",
    "# We downsampled the training set to 50000: we sample 10000 examples per label\n",
    "de_train = de_train.groupby('label').apply(lambda x: x.sample(n=10000, replace=False) if len(x) >= 10000 else x).reset_index(drop=True)\n",
    "# We downsampled the val set to 500: we sample 100 examples per label\n",
    "de_test = de_test.groupby('label').apply(lambda x: x.sample(n=100, replace=False) if len(x) >= 100 else x).reset_index(drop=True)\n",
    "# We downsampled the test set to 500: we sample 100 examples per label\n",
    "de_val = de_val.groupby('label').apply(lambda x: x.sample(n=100, replace=False) if len(x) >= 100 else x).reset_index(drop=True)\n",
    "\n",
    "# Convert in datasets object\n",
    "# de_train = Dataset.from_pandas(de_train)\n",
    "de_test = Dataset.from_pandas(de_test)\n",
    "# de_val = Dataset.from_pandas(de_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-14T23:37:20.295785Z",
     "iopub.status.busy": "2024-12-14T23:37:20.295529Z",
     "iopub.status.idle": "2024-12-14T23:37:27.249905Z",
     "shell.execute_reply": "2024-12-14T23:37:27.249202Z",
     "shell.execute_reply.started": "2024-12-14T23:37:20.295760Z"
    },
    "id": "Nm0LiYJU81wY"
   },
   "outputs": [],
   "source": [
    "#Loading the German corpus as pandas dataframe\n",
    "es_train = pd.DataFrame(ds_es[\"train\"])\n",
    "es_test = pd.DataFrame(ds_es[\"test\"])\n",
    "es_val = pd.DataFrame(ds_es[\"validation\"])\n",
    "\n",
    "# We downsampled the training set to 50000: we sample 10000 examples per label\n",
    "es_train = es_train.groupby('label').apply(lambda x: x.sample(n=10000, replace=False) if len(x) >= 10000 else x).reset_index(drop=True)\n",
    "# We downsampled the test set to 500: we sample 100 examples per label\n",
    "es_test = es_test.groupby('label').apply(lambda x: x.sample(n=100, replace=False) if len(x) >= 100 else x).reset_index(drop=True)\n",
    "# We downsampled the val set to 500: we sample 100 examples per label\n",
    "es_val = es_val.groupby('label').apply(lambda x: x.sample(n=100, replace=False) if len(x) >= 100 else x).reset_index(drop=True)\n",
    "\n",
    "#convert in datasets object\n",
    "# es_train = Dataset.from_pandas(es_train)\n",
    "es_test = Dataset.from_pandas(es_test)\n",
    "# es_val = Dataset.from_pandas(es_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-14T23:37:27.251934Z",
     "iopub.status.busy": "2024-12-14T23:37:27.251630Z",
     "iopub.status.idle": "2024-12-14T23:37:27.643576Z",
     "shell.execute_reply": "2024-12-14T23:37:27.642910Z",
     "shell.execute_reply.started": "2024-12-14T23:37:27.251906Z"
    },
    "id": "zMeuSBQW81wZ"
   },
   "outputs": [],
   "source": [
    "# Concatenate the all non-English corpora as pandas dataframe\n",
    "non_english_train = pd.concat([fr_train, de_train, es_train])\n",
    "non_english_val = pd.concat([fr_val, de_val, es_val])\n",
    "\n",
    "# Convert to datasets object\n",
    "non_english_train = Dataset.from_pandas(non_english_train)\n",
    "non_english_val = Dataset.from_pandas(non_english_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_WG18u3j81wZ"
   },
   "source": [
    "#### 2.2.2. Preprocessing\n",
    "\n",
    "In order to feed the MARC corpora to the our models, we first proceed to the its tokenization. It is important to that we use the tokenizer of the model we intend to further fine-tune. For this reason, we load the tokenizer of `bert-base-multilingual-cased` as it is the model we will be using for fine-tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "referenced_widgets": [
      "ceb4170700cd4d0c9932997fec6f4a58",
      "91704783f1954df6ac56fff65a0608ca",
      "b9dabcf1f0cf4fc88cfb422864966085",
      "cb915c36693745579a920d1693bf2f41"
     ]
    },
    "execution": {
     "iopub.execute_input": "2024-12-14T23:37:27.645112Z",
     "iopub.status.busy": "2024-12-14T23:37:27.644756Z",
     "iopub.status.idle": "2024-12-14T23:37:29.248163Z",
     "shell.execute_reply": "2024-12-14T23:37:29.247386Z",
     "shell.execute_reply.started": "2024-12-14T23:37:27.645076Z"
    },
    "id": "L_T1CR7X81wZ",
    "outputId": "59f34d67-c626-46c5-bd58-c9cb97b6cb3d"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ceb4170700cd4d0c9932997fec6f4a58",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/49.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "91704783f1954df6ac56fff65a0608ca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/625 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b9dabcf1f0cf4fc88cfb422864966085",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/996k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cb915c36693745579a920d1693bf2f41",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.96M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Load the tokenizer\n",
    "model_name = \"bert-base-multilingual-cased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "referenced_widgets": [
      "878769630f674dfcb198c7f2ea244c5e",
      "7275b30f5bfd4eab9151b9d40c53b62d",
      "0da2d70b3d5b46039a76dd515d416cd6",
      "8141cb9f3caa4f21babcceddf4a682db",
      "b376d93023e242799d66bdc70e91aa93"
     ]
    },
    "execution": {
     "iopub.execute_input": "2024-12-14T23:37:29.249686Z",
     "iopub.status.busy": "2024-12-14T23:37:29.249316Z",
     "iopub.status.idle": "2024-12-14T23:38:14.806231Z",
     "shell.execute_reply": "2024-12-14T23:38:14.805349Z",
     "shell.execute_reply.started": "2024-12-14T23:37:29.249621Z"
    },
    "id": "ImktaQZz81wZ",
    "outputId": "2eb2e6ae-c6f5-4cff-e434-7ffa284dc979"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "878769630f674dfcb198c7f2ea244c5e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/100000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7275b30f5bfd4eab9151b9d40c53b62d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/500 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0da2d70b3d5b46039a76dd515d416cd6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/500 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8141cb9f3caa4f21babcceddf4a682db",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/150000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b376d93023e242799d66bdc70e91aa93",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1500 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Tokenization\n",
    "#This is a helper function for the tokenization\n",
    "def tokenize_function(example):\n",
    "  #\n",
    "  return tokenizer(example[\"text\"], padding=\"max_length\", truncation=True, max_length=256)\n",
    "\n",
    "#Tokenization of the English set\n",
    "en_train_tokenized = en_train.map(tokenize_function, batched=True)\n",
    "en_val_tokenized = en_val.map(tokenize_function, batched=True)\n",
    "en_test_tokenized = en_test.map(tokenize_function, batched=True)\n",
    "\n",
    "#Tokenization of the non-English set\n",
    "non_english_train_tokenized = non_english_train.map(tokenize_function, batched=True)\n",
    "non_english_val_tokenized = non_english_val.map(tokenize_function, batched=True)\n",
    "\n",
    "#Tokenization of the each of the test set of the non English test set\n",
    "# fr_test_tokenized = fr_test.map(tokenize_function, batched=True)\n",
    "# de_test_tokenized = de_test.map(tokenize_function, batched=True)\n",
    "# es_test_tokenized = es_test.map(tokenize_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-14T23:38:14.807803Z",
     "iopub.status.busy": "2024-12-14T23:38:14.807512Z",
     "iopub.status.idle": "2024-12-14T23:38:14.836353Z",
     "shell.execute_reply": "2024-12-14T23:38:14.835627Z",
     "shell.execute_reply.started": "2024-12-14T23:38:14.807773Z"
    },
    "id": "9eVYcOks81wZ"
   },
   "outputs": [],
   "source": [
    "del en_train, en_val, en_test, fr_train, fr_val, fr_test, es_train, es_val, es_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F1ntHh_JbrAg"
   },
   "source": [
    "---\n",
    "\n",
    "## 3. Machine learning model\n",
    "\n",
    "### 3.1. Model training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yCXl1cVP81wZ"
   },
   "source": [
    "In this section, we full fine-tune the `bert-base-multilingual-cased` on the English corpus. The model is trained for 2 epochs with a batch size of 32, and the best model is selected based on accuracy. Then after training, the model is evaluated on the English validation set using accuracy as the evaluation metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "referenced_widgets": [
      "4694bdd4fbee466e9fab99d537d4a680",
      "514793c6ca8f44509acaef34ff42c530"
     ]
    },
    "execution": {
     "iopub.execute_input": "2024-12-14T23:38:14.838495Z",
     "iopub.status.busy": "2024-12-14T23:38:14.838196Z",
     "iopub.status.idle": "2024-12-15T00:19:25.800804Z",
     "shell.execute_reply": "2024-12-15T00:19:25.800044Z",
     "shell.execute_reply.started": "2024-12-14T23:38:14.838469Z"
    },
    "id": "1fk_7doW81wZ",
    "outputId": "e794268a-df42-42fa-b9ff-85e0cd0424e1"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4694bdd4fbee466e9fab99d537d4a680",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/714M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "514793c6ca8f44509acaef34ff42c530",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading builder script:   0%|          | 0.00/4.20k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit:"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "  ········\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.18.7"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/kaggle/working/wandb/run-20241214_233828-l9nhbi0u</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/clemsadand-stellenbosch-university/huggingface/runs/l9nhbi0u' target=\"_blank\">./results</a></strong> to <a href='https://wandb.ai/clemsadand-stellenbosch-university/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/clemsadand-stellenbosch-university/huggingface' target=\"_blank\">https://wandb.ai/clemsadand-stellenbosch-university/huggingface</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/clemsadand-stellenbosch-university/huggingface/runs/l9nhbi0u' target=\"_blank\">https://wandb.ai/clemsadand-stellenbosch-university/huggingface/runs/l9nhbi0u</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3125' max='3125' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3125/3125 40:54, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.840600</td>\n",
       "      <td>0.820072</td>\n",
       "      <td>0.650000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=3125, training_loss=0.9139765771484375, metrics={'train_runtime': 2463.7039, 'train_samples_per_second': 40.589, 'train_steps_per_second': 1.268, 'total_flos': 1.31559071232e+16, 'train_loss': 0.9139765771484375, 'epoch': 1.0})"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#  code to train the transformer based model on the training set and evaluate the performance on the validation set here\n",
    "#Load the model\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=5)\n",
    "\n",
    "#Full Fine-Tuning on English\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    save_total_limit=1,\n",
    "    per_device_train_batch_size=32,\n",
    "    per_device_eval_batch_size=32,\n",
    "    num_train_epochs=1,\n",
    "    weight_decay=0.01,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"accuracy\",\n",
    "    greater_is_better=True,\n",
    ")\n",
    "\n",
    "# Load the accuracy metric\n",
    "accuracy_metric = evaluate.load(\"accuracy\")\n",
    "\n",
    "# Define a function to compute metrics\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    preds = np.argmax(logits, axis=1)\n",
    "    accuracy = accuracy_score(labels, preds)\n",
    "    return {\"accuracy\": accuracy}\n",
    "\n",
    "# Set up the Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=en_train_tokenized,\n",
    "    eval_dataset=en_val_tokenized,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "trainer.train()#resume_from_checkpoint=\"/kaggle/working/results/checkpoint-4500\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nlO8RVuHcmAh"
   },
   "source": [
    "### 3.2 Hyperparameter optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-15T00:19:25.802107Z",
     "iopub.status.busy": "2024-12-15T00:19:25.801859Z",
     "iopub.status.idle": "2024-12-15T01:00:30.509530Z",
     "shell.execute_reply": "2024-12-15T01:00:30.508682Z",
     "shell.execute_reply.started": "2024-12-15T00:19:25.802084Z"
    },
    "id": "FrhJ0auM81wZ",
    "outputId": "644bb884-3aed-4100-cb82-567da7685076"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3125' max='3125' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3125/3125 41:02, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.783900</td>\n",
       "      <td>0.817608</td>\n",
       "      <td>0.662000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=3125, training_loss=0.7339735400390625, metrics={'train_runtime': 2463.5479, 'train_samples_per_second': 40.592, 'train_steps_per_second': 1.268, 'total_flos': 1.31559071232e+16, 'train_loss': 0.7339735400390625, 'epoch': 1.0})"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#  code for hyperparameter optimization here\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    save_total_limit=1,\n",
    "    per_device_train_batch_size=32,\n",
    "    per_device_eval_batch_size=32,\n",
    "    num_train_epochs=1,\n",
    "    weight_decay=0.01,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"accuracy\",\n",
    "    greater_is_better=True,\n",
    ")\n",
    "\n",
    "# Load the accuracy metric\n",
    "accuracy_metric = evaluate.load(\"accuracy\")\n",
    "\n",
    "# Define a function to compute metrics\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    preds = np.argmax(logits, axis=1)\n",
    "    accuracy = accuracy_score(labels, preds)\n",
    "    return {\"accuracy\": accuracy}\n",
    "\n",
    "# Set up the Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=en_train_tokenized,\n",
    "    eval_dataset=en_val_tokenized,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "trainer.train()#resume_from_checkpoint=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-15T01:00:30.512692Z",
     "iopub.status.busy": "2024-12-15T01:00:30.512394Z",
     "iopub.status.idle": "2024-12-15T01:00:33.021054Z",
     "shell.execute_reply": "2024-12-15T01:00:33.020131Z",
     "shell.execute_reply.started": "2024-12-15T01:00:30.512655Z"
    },
    "id": "xYRwNyYm81wa"
   },
   "outputs": [],
   "source": [
    "trainer.save_model(\"./baseline_model\")\n",
    "trainer.save_state()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1EzCYTnfcrvN"
   },
   "source": [
    "### 3.3. Evaluation on test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-15T01:00:33.022430Z",
     "iopub.status.busy": "2024-12-15T01:00:33.022161Z",
     "iopub.status.idle": "2024-12-15T01:00:36.792500Z",
     "shell.execute_reply": "2024-12-15T01:00:36.791906Z",
     "shell.execute_reply.started": "2024-12-15T01:00:33.022404Z"
    },
    "id": "t--DW1x-81wa",
    "outputId": "95d29a9b-65ff-45fc-f4ce-c235f3ee12cd"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='16' max='16' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [16/16 00:03]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.8470733761787415,\n",
       " 'eval_accuracy': 0.636,\n",
       " 'eval_runtime': 3.76,\n",
       " 'eval_samples_per_second': 132.979,\n",
       " 'eval_steps_per_second': 4.255,\n",
       " 'epoch': 1.0}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# code to evaluate the final model on the test set here\n",
    "trainer.evaluate(eval_dataset=en_test_tokenized)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zzThmhszBKjg"
   },
   "source": [
    "### 3.4. Cross-lingual experiments\n",
    "\n",
    "We full fine-tune the `bert-base-multilingual-cased` model on non-English data (French, German, Spanish) for 2 epochs with a batch size of 32, selecting the best model based on accuracy. After training, the model is evaluated on the English validation set, benefiting from zero-shot cross-lingual transfer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-15T01:00:36.794052Z",
     "iopub.status.busy": "2024-12-15T01:00:36.793700Z",
     "iopub.status.idle": "2024-12-15T03:03:40.087702Z",
     "shell.execute_reply": "2024-12-15T03:03:40.086863Z",
     "shell.execute_reply.started": "2024-12-15T01:00:36.794013Z"
    },
    "id": "QMzbO_aq81wa",
    "outputId": "8a3c9fdf-3bbf-40c3-f432-a9521e7c56b5"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='9376' max='9376' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [9376/9376 2:03:00, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.945900</td>\n",
       "      <td>0.908367</td>\n",
       "      <td>0.600000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.824500</td>\n",
       "      <td>0.891850</td>\n",
       "      <td>0.613333</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=9376, training_loss=0.9326762762492834, metrics={'train_runtime': 7381.7222, 'train_samples_per_second': 40.641, 'train_steps_per_second': 1.27, 'total_flos': 3.94677213696e+16, 'train_loss': 0.9326762762492834, 'epoch': 2.0})"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# code to train and evaluate the cross-lingual model\n",
    "\n",
    "#Full Fine-Tuning on non-English (French, Deutch, Spanish)\n",
    "\n",
    "model_multi = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=5)\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results_multi\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    save_total_limit=2,\n",
    "    per_device_train_batch_size=32,\n",
    "    per_device_eval_batch_size=32,\n",
    "    num_train_epochs=2,\n",
    "    weight_decay=0.01,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"accuracy\",\n",
    "    greater_is_better=True,\n",
    ")\n",
    "\n",
    "# Load the accuracy metric\n",
    "accuracy_metric = evaluate.load(\"accuracy\")\n",
    "\n",
    "# Define a function to compute metrics\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    preds = np.argmax(logits, axis=1)\n",
    "    accuracy = accuracy_score(labels, preds)\n",
    "    return {\"accuracy\": accuracy}\n",
    "\n",
    "\n",
    "# Set up the Trainer\n",
    "trainer_non = Trainer(\n",
    "    model=model_multi,\n",
    "    args=training_args,\n",
    "    train_dataset=non_english_train_tokenized,\n",
    "    eval_dataset=non_english_val_tokenized,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "trainer_non.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-15T03:03:40.089135Z",
     "iopub.status.busy": "2024-12-15T03:03:40.088849Z",
     "iopub.status.idle": "2024-12-15T03:03:43.924669Z",
     "shell.execute_reply": "2024-12-15T03:03:43.923825Z",
     "shell.execute_reply.started": "2024-12-15T03:03:40.089103Z"
    },
    "id": "59A4jAlB81wa",
    "outputId": "87537c9c-9bd7-4de3-8dbb-9323268e6ffb"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='16' max='16' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [16/16 00:03]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 1.0888502597808838,\n",
       " 'eval_accuracy': 0.58,\n",
       " 'eval_runtime': 3.8244,\n",
       " 'eval_samples_per_second': 130.739,\n",
       " 'eval_steps_per_second': 4.184,\n",
       " 'epoch': 2.0}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer_non.evaluate(eval_dataset=en_test_tokenized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-15T03:03:43.926180Z",
     "iopub.status.busy": "2024-12-15T03:03:43.925828Z",
     "iopub.status.idle": "2024-12-15T03:03:46.681565Z",
     "shell.execute_reply": "2024-12-15T03:03:46.680874Z",
     "shell.execute_reply.started": "2024-12-15T03:03:43.926142Z"
    },
    "id": "U_vdxM-981wa"
   },
   "outputs": [],
   "source": [
    "trainer_non.save_model(\"./model_multi\")\n",
    "trainer_non.save_state()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x7ylOS8FdYZ5"
   },
   "source": [
    "---\n",
    "\n",
    "## 4. Results and summary\n",
    "\n",
    "### 4.1 Corpus insights\n",
    "\n",
    "<!-- Briefly discuss what was learned about the corpus and its annotation -->\n",
    "\n",
    "The MARC corpus contains Amazon reviews in multiple languages including English, French, German and Spanish labeled with star ratings, ranging from 1 to 5 stars. The dataset is carefully balanced to ensure each star rating is equally represented, with 20% of reviews per rating. The annotations in the corpus are the star ratings, which represent the target variable for text classification tasks. The corpus is designed for multilingual tasks, providing a resource for training and evaluating models on a variety of languages.\n",
    "\n",
    "### 4.2 Results\n",
    "\n",
    "<!-- Briefly summarize  results -->\n",
    "The results show that fine-tuning the `bert-base-multilingual-cased` model on English data achieved an accuracy of 63.6% on the English test set. When fine-tuned on non-English data (French, German, Spanish), the model evaluated on the English test set using zero-shot cross-lingual transfer achieved an accuracy of 58%.\n",
    "\n",
    "### 4.3 Relation to random baseline / expected performance / state of the art\n",
    "\n",
    "<!-- Compare obtained results with the random and state-of-the-art performance -->\n",
    "Compared to the random baseline, which would yield an accuracy of around **20%** for a five-class classification task, the fine-tuned `bert-base-multilingual-cased` model achieved **63.6%** accuracy when trained on English data, a substantial improvement. For the zero-shot cross-lingual transfer model, fine-tuned on non-English data, the accuracy on the English test set was **58%**, also significantly better than the random baseline.\n",
    "\n",
    "In comparison to recent state-of-the-art performance, mBERT achieves **59.2%** accuracy in fully supervised fine-grained classification and **44.0%** accuracy in zero-shot cross-lingual transfer. While our fine-tuned model's performance on English data (63.6%) surpasses mBERT's zero-shot accuracy (58%), it still lags behind mBERT’s fully supervised accuracy (59.2%). This indicates that while the model performs well, it falls slightly short of the highest state-of-the-art results, suggesting room for improvement through further optimization.\n",
    "\n",
    "---\n",
    "\n",
    "## 5. Zero-shot with a generative language model\n",
    "\n",
    "### 5.1. and 5.2. Model and Data selection\n",
    "\n",
    "<!-- Briefly describe which model was used and why. Also, describe how the test data was downsampled, include relevant code. -->\n",
    "We selected `mistralai/Mistral-7B-Instruct-v0.2` for this classification task due to its instruction-tuned nature, which makes it highly efficient in following natural language instructions. This model, with 7 billion parameters, strikes a balance between computational efficiency and high performance, making it an ideal choice for zero-shot learning tasks like text classification. Its ability to understand and process complex instructions without the need for task-specific fine-tuning allows for quick deployment and effective handling of diverse inputs. The model is optimized for general language understanding and generation, ensuring robust performance across various tasks, including text classification, which is essential for accurately categorizing reviews into predefined sentiment categories.\n",
    "\n",
    "For the testing process, the data was downsampled to ensure computational efficiency and maintain a balanced representation of each class. Given that real-world datasets can often be large and imbalanced, we used a method to ensure that each class had a sufficient number of samples for testing. Specifically, we applied the following approach: for each class in the dataset, if the class had at least 100 samples, we randomly sampled 100 reviews from it (with replacement if necessary). If a class had fewer than 100 samples, we kept all the reviews from that class. This strategy was implemented using the `groupby` and `apply` functions in Pandas. After downsampling, the dataset was reset and shuffled to eliminate any inherent ordering bias, ensuring a more generalized evaluation. This approach allowed for an efficient and balanced test set, reducing computational load while still capturing the diversity across all sentiment categories.\n",
    "\n",
    "### 5.3. Prompt design\n",
    "\n",
    "<!-- Include  final prompt here. Also, explain here all prompt engineering insights was learned while completing the tasks or project. -->\n",
    "#### Final Prompt\n",
    "\n",
    "Here is the final prompt:\n",
    "\n",
    "```\n",
    "I want you to perform a classification task for user reviews of products they bought.\n",
    "You have to classify them on a scale of 0 to 4 using the integers 0, 1, 2, 3, 4.\n",
    "Classify the given review into one of the following categories based on sentiment:\n",
    "- 0: Very negative (the user is extremely dissatisfied with the product).\n",
    "- 1: Negative (the user is dissatisfied with the product).\n",
    "- 2: Neutral (the user feels indifferent about the product).\n",
    "- 3: Positive (the user is satisfied with the product).\n",
    "- 4: Very positive (the user is extremely satisfied with the product).\n",
    "\n",
    "Input Format:\n",
    "A review as plain text written between these symbols *** and ***.\n",
    "\n",
    "Output Format:\n",
    "Return only one of the following numbers: 0, 1, 2, or 4.\n",
    "No additional text. Don't justify your answer. Don't comment your answer.\n",
    "Your answer should not exceed one word, and this word should be an integer.\n",
    "Just send the integer representing the category you decide after analyzing the review.\n",
    "\n",
    "Here is the review to classify:\n",
    "Review: ***{review}***\n",
    "```\n",
    "\n",
    "#### Prompt Engineering Insights\n",
    "\n",
    "While completing this project, several prompt engineering insights emerged:\n",
    "\n",
    "1. **Clear Instructions**: The model needs clear and structured instructions. The prompt outlines the specific task (classifying reviews on a sentiment scale) and defines each category in detail. This clarity helps the model understand the task and reduces the chances of ambiguous responses.\n",
    "\n",
    "2. **Concise Output Formatting**: The prompt ensures that the output is constrained to a single integer, avoiding unnecessary explanation or additional text. This is critical for maintaining simplicity and consistency in the model’s output, particularly for downstream processing.\n",
    "\n",
    "3. **Explicit Constraints**: By specifying \"No additional text\" and \"Return only one of the following numbers,\" we limit the possible range of responses and guide the model toward the desired format. This was crucial for controlling the output in a structured manner.\n",
    "\n",
    "4. **Controlled Generation Parameters**: Using parameters like `temperature`, `top_k`, and `top_p` allows for controlling the creativity and diversity of the model's output. In this case, a lower temperature ensures more deterministic outputs, which is important for a classification task where consistency is key.\n",
    "\n",
    "5. **Review as Input**: The formatting of the review as `***review***` within the prompt emphasizes that the review text is the focal point for the model, ensuring that the model focuses on classifying the content correctly based on the provided sentiment categories.\n",
    "\n",
    "6. **Iterative Refinement**: Testing the prompt with various types of reviews revealed that minor adjustments (like temperature or response formatting) were needed to improve the accuracy and consistency of the output. It highlighted the importance of testing and fine-tuning the prompt structure for optimal results.\n",
    "\n",
    "These prompt engineering techniques were critical in obtaining reliable and consistent classifications from the generative model.\n",
    "\n",
    "### 5.4. Generate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "referenced_widgets": [
      "db8c04178e934a67aac879008ea3db73"
     ]
    },
    "collapsed": true,
    "execution": {
     "iopub.execute_input": "2024-12-15T05:45:54.357624Z",
     "iopub.status.busy": "2024-12-15T05:45:54.357117Z",
     "iopub.status.idle": "2024-12-15T05:55:39.713770Z",
     "shell.execute_reply": "2024-12-15T05:55:39.712855Z",
     "shell.execute_reply.started": "2024-12-15T05:45:54.357585Z"
    },
    "id": "TwGT-LMQ81wb",
    "jupyter": {
     "outputs_hidden": true
    },
    "outputId": "bcec2902-53f6-4f8f-8224-16ec049c68ae"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "db8c04178e934a67aac879008ea3db73",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/500 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    }
   ],
   "source": [
    "#  code to run the generative model and extract predictions from the model output.\n",
    "\n",
    "#Load Mistral7B\n",
    "\n",
    "#Easier of doing, not the best:\n",
    "#Replace my_hugging_face_token by your own token and then\n",
    "#make sure that you have been granted access to the mistral 7B model\n",
    "#Go to https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.2 for that\n",
    "os.environ['HF_AUTH_TOKEN'] = 'my_hugging_face_token'\n",
    "\n",
    "#Model's name\n",
    "MODEL_NAME = 'mistralai/Mistral-7B-Instruct-v0.2'\n",
    "\n",
    "#Setting for quantization\n",
    "quantization_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    ")\n",
    "\n",
    "#Loading mistral's tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "      MODEL_NAME,\n",
    "      padding_side='left',\n",
    "      # token=userdata.get('hugging_face_secret'),\n",
    "      use_auth_token=os.getenv('HF_AUTH_TOKEN'),\n",
    "    )\n",
    "\n",
    "#Loading mistral\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "      MODEL_NAME,\n",
    "      # token=userdata.get('hugging_face_secret'),\n",
    "      use_auth_token=os.getenv('HF_AUTH_TOKEN'),\n",
    "      device_map=\"auto\",\n",
    "      quantization_config=quantization_config\n",
    "    )\n",
    "\n",
    "model.config.pad_token_id = model.config.eos_token_id\n",
    "\n",
    "\n",
    "#Instantiate Text Generation Pipeline\n",
    "pipe = pipeline(\"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer\n",
    ")\n",
    "\n",
    "\n",
    "#Function to substract the output from\n",
    "def get_rate(output):\n",
    "    \"\"\"output: a string\n",
    "    returns: number between -1, 0, 1, 2, 4\n",
    "    -1 means the string does have any digit\n",
    "    \"\"\"\n",
    "    match = re.search(r\"\\d\", output)\n",
    "    if match:\n",
    "        number = int(match.group())\n",
    "        return number\n",
    "    return -1\n",
    "\n",
    "#Test the function\n",
    "# get_rate(\"The product code is 1\")\n",
    "\n",
    "def zero_shot_classifier(review):\n",
    "\n",
    "    conversation = [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": (\n",
    "                 f\"\"\"I want you to perform a classification task for user reviews of products they bought.\n",
    "                And you have to classify them on a scale of 0 to 4 using the integers 0, 1, 2, 3, 4.\n",
    "                Classify the given review into one of the following categories based on sentiment:\\n\n",
    "                - 0: Very negative (the user is extremely dissatisfied with the product).\\n\n",
    "                - 1: Negative (the user is dissatisfied with the product).\\n\n",
    "                - 2: Neutral (the user feels indifferent about the product).\\n\n",
    "                - 3: Positive (the user is satisfied with the product).\\n\n",
    "                - 4: Very positive (the user is extremely satisfied with the product).\\n\\n\n",
    "                Input Format:\\n\n",
    "                A review as plain text wrote between these symboles *** and ***.\\n\\n\n",
    "                Output Format:\\n\n",
    "                Return only one the following number 0, 1, 2, or 4.\\n\n",
    "                No additional text. Don't justify your answer. Don't comment your answer.\\n\n",
    "                Your answer should not exceed one word and this word shoud be an integer.\\n\n",
    "                Just send the integer representing the category you decide after analyzing the review.\\n\\n\n",
    "                Here is the review to classify:\\nReview: ***{review}***\"\"\"\n",
    "            ),\n",
    "        }\n",
    "    ]\n",
    "\n",
    "    temperature = 0.7\n",
    "    top_k = 50\n",
    "    top_p = 0.9\n",
    "    # output = pipe(conversation, max_new_tokens=50)\n",
    "    output = pipe(\n",
    "        conversation,\n",
    "        max_new_tokens=5,\n",
    "        temperature=temperature,\n",
    "        top_k=top_k,\n",
    "        top_p=top_p,\n",
    "    )\n",
    "\n",
    "    out = output[0][\"generated_text\"][-1][\"content\"]\n",
    "    return get_rate(out)\n",
    "\n",
    "# Testing our function\n",
    "# for i in [1, 5, 19, 48, 100, 182, 200, 401, 453]:\n",
    "#     print(i,\":\",zero_shot_classifier(en_test[i][\"text\"]))\n",
    "\n",
    "\n",
    "#Inference\n",
    "en_test_copy = en_test_copy.map(lambda example: {\"predicted_label\": zero_shot_classifier(example[\"text\"])})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "execution": {
     "iopub.execute_input": "2024-12-15T05:42:48.692217Z",
     "iopub.status.busy": "2024-12-15T05:42:48.691963Z",
     "iopub.status.idle": "2024-12-15T05:42:48.728842Z",
     "shell.execute_reply": "2024-12-15T05:42:48.728067Z",
     "shell.execute_reply.started": "2024-12-15T05:42:48.692189Z"
    },
    "id": "22-KmHLV81wb",
    "jupyter": {
     "outputs_hidden": true
    },
    "outputId": "921bdf9a-ca13-4871-9fdc-3ea6514256c4"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>label_text</th>\n",
       "      <th>predicted_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>490</th>\n",
       "      <td>en_0528033</td>\n",
       "      <td>Love this stool\\n\\nI like the size and the gre...</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>491</th>\n",
       "      <td>en_0458455</td>\n",
       "      <td>This is an amazing product! It smells amazing\\...</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>492</th>\n",
       "      <td>en_0227165</td>\n",
       "      <td>Best cold brew!\\n\\nAmazing flavor with just th...</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>493</th>\n",
       "      <td>en_0639833</td>\n",
       "      <td>Great\\n\\nThe book is very good, the transport ...</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>494</th>\n",
       "      <td>en_0249421</td>\n",
       "      <td>A MUST BUY\\n\\nA must buy for any Home</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>495</th>\n",
       "      <td>en_0344435</td>\n",
       "      <td>Five Stars\\n\\nSuper nice box!!! Got it next da...</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>496</th>\n",
       "      <td>en_0989620</td>\n",
       "      <td>5 Stars\\n\\nLove it! As with all Tree to Tub Pr...</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497</th>\n",
       "      <td>en_0926260</td>\n",
       "      <td>Very practical and convenient\\n\\nCome with pre...</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498</th>\n",
       "      <td>en_0065576</td>\n",
       "      <td>Adorable. Really.\\n\\nThese are really cute. I ...</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>499</th>\n",
       "      <td>en_0858705</td>\n",
       "      <td>👍🏼\\n\\nLove this book it came in great conditio...</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             id                                               text  label  \\\n",
       "490  en_0528033  Love this stool\\n\\nI like the size and the gre...      4   \n",
       "491  en_0458455  This is an amazing product! It smells amazing\\...      4   \n",
       "492  en_0227165  Best cold brew!\\n\\nAmazing flavor with just th...      4   \n",
       "493  en_0639833  Great\\n\\nThe book is very good, the transport ...      4   \n",
       "494  en_0249421              A MUST BUY\\n\\nA must buy for any Home      4   \n",
       "495  en_0344435  Five Stars\\n\\nSuper nice box!!! Got it next da...      4   \n",
       "496  en_0989620  5 Stars\\n\\nLove it! As with all Tree to Tub Pr...      4   \n",
       "497  en_0926260  Very practical and convenient\\n\\nCome with pre...      4   \n",
       "498  en_0065576  Adorable. Really.\\n\\nThese are really cute. I ...      4   \n",
       "499  en_0858705  👍🏼\\n\\nLove this book it came in great conditio...      4   \n",
       "\n",
       "    label_text  predicted_label  \n",
       "490          4                3  \n",
       "491          4                4  \n",
       "492          4                4  \n",
       "493          4                3  \n",
       "494          4                4  \n",
       "495          4                4  \n",
       "496          4                4  \n",
       "497          4                3  \n",
       "498          4                4  \n",
       "499          4                4  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Visualizing Mistral's prediction\n",
    "pd.DataFrame(en_test_copy).tail(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-15T05:55:39.895801Z",
     "iopub.status.busy": "2024-12-15T05:55:39.895438Z",
     "iopub.status.idle": "2024-12-15T05:55:39.906247Z",
     "shell.execute_reply": "2024-12-15T05:55:39.905339Z",
     "shell.execute_reply.started": "2024-12-15T05:55:39.895759Z"
    },
    "id": "rx7fAg6c81wb",
    "outputId": "5d25f4e8-2375-4ace-aadf-51e6d705b783"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 47.60%\n"
     ]
    }
   ],
   "source": [
    "#Function to calculate accuracy\n",
    "def calculate_accuracy(predictions, true_labels):\n",
    "    correct = sum([1 if p == t else 0 for p, t in zip(predictions, true_labels)])\n",
    "    accuracy = correct / len(true_labels)\n",
    "    return accuracy\n",
    "\n",
    "#list of predicted and true labels\n",
    "predicted_labels = en_test_copy[\"predicted_label\"]\n",
    "true_labels = en_test_copy[\"label\"]\n",
    "\n",
    "#Calculate the accuracy\n",
    "accuracy = calculate_accuracy(predicted_labels, true_labels)\n",
    "\n",
    "print(f\"Accuracy: {accuracy * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4ghO4JemeFKK"
   },
   "source": [
    "### 5.5. Evaluation and results\n",
    "\n",
    "<!-- Briefly summarize results -->\n",
    "Comparing the fine-tuned `bert-base-multilingual-cased` model, which achieved **63.6%** accuracy on the English test set, to the zero-shot prompting of the Mistral 7B model yielded an accuracy of **47.60%**. While both models demonstrated good performance, the `bert-base-multilingual-cased` model outperformed Mistral 7B, highlighting the advantage of task-specific fine-tuning over zero-shot learning. This result aligns with previous findings, where fine-tuned models, such as mBERT, showed significantly better accuracy than zero-shot cross-lingual models (59.2% for mBERT in a fully supervised task). Thus, although Mistral 7B performed well in zero-shot classification, it still trails behind the accuracy achieved by fine-tuned models like `bert-base-multilingual-cased`."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [],
   "dockerImageVersionId": 30805,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
